import sys
import os

# Ajustar path para encontrar src
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

from src.sre.generation.llm_client import get_rag_model

# 1. Instanciar el modelo
print("ğŸš€ Iniciando modelo RAG...")
rag = get_rag_model()

# 2. Datos simulados (como si vinieran de Pinecone)
context_chunks = [
    "DB-SI 4: Los extintores deben colocarse a una altura mÃ¡xima de 1.20m.",
    "DB-SI 4: La distancia mÃ¡xima entre extintores serÃ¡ de 15 metros."
]

query = "Â¿A quÃ© altura se ponen los extintores?"

# 3. Generar respuesta
print(f"â“ Pregunta: {query}")
print("â³ Generando respuesta (llamando a OpenAI)...")

try:
    result = rag.generate_response(
        query=query, 
        context_chunks=context_chunks,
        run_name="test-manual-extintores"
    )

    print("\nâœ… RESPUESTA RECIBIDA:")
    print(result["answer"])
    print("\nğŸ“Š MÃ‰TRICAS:")
    print(result["metrics"])
    print(f"\nğŸ”— MLflow Run ID: {result['metadata']['run_id']}")
    print("ğŸ‘‰ Revisa http://localhost:5000 para ver el log completo.")

except Exception as e:
    print(f"\nâŒ ERROR: {e}")
    print("AsegÃºrate de tener OPENAI_API_KEY correcta en tu .env")